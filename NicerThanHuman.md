[ ]

[](https://ojs.aaai.org/plugins/generic/pdfJsViewer/pdf.js/web/viewer.html?file=https%3A%2F%2Fojs.aaai.org%2Findex.php%2FICWSM%2Farticle%2Fdownload%2F35829%2F37983%2F39897#page=1&zoom=110,-125,792 "Current view (copy or open in new window)")

    Automatic Zoom                     Actual Size                     Page Fit                     Page Width                     110%                     50%                     75%                     100%                     125%                     150%                     200%                     300%                     400%

Nicer than Humans: How Do Large Language Models Behave in the Prisoner’sDilemma?Nicol ́o Fontana1, Francesco Pierri1, Luca Maria Aiello2,31Politecnico di Milano, Italy2IT University of Copenhagen, Denmark3Pioneer Centre for AI, Denmarknicolo.fontana@mail.polimi.it, francesco.pierri@polimi.it, luai@itu.dkAbstractThe  behavior  of  Large  Language  Models  (LLMs)  as  artifi-cial social agents is largely unexplored, and we still lack ex-tensive evidence of how these agents react to simple socialstimuli.  Testing  the  behavior  of  AI  agents  in  classic  GameTheory experiments provides a promising theoretical frame-work  for  evaluating  the  norms  and  values  of  these  agentsin  archetypal social  situations.  In  this  work,  we  investigatethe  cooperative  behavior  of  three  LLMs  (Llama2,  Llama3,and GPT3.5) when playing the Iterated Prisoner’s Dilemmaagainst random adversaries displaying various levels of hos-tility. We introduce a systematic methodology to evaluate anLLM’s comprehension of the game rules and its capability toparse historical gameplay logs for decision-making. We con-ducted simulations of games lasting for 100 rounds and an-alyzed the LLMs’ decisions in terms of dimensions definedin the behavioral economics literature. We find that all mod-els tend not to initiate defection but act cautiously, favoringcooperation over defection only when the opponent’s defec-tion rate is low. Overall, LLMs behave at least as coopera-tively as the typical human player, although our results indi-cate some substantial differences among models. In particu-lar, Llama2 and GPT3.5 are more cooperative than humans,and especially forgiving and non-retaliatory for opponent de-fection  rates  below  30%.  More  similar  to  humans,  Llama3exhibits consistently uncooperative and exploitative behaviorunless the opponent always cooperates. Our systematic ap-proach to the study of LLMs in game theoretical scenarios isa step towards using these simulations to inform practices ofLLM auditing and alignment.1    IntroductionLarge  Language  Models  (LLMs)  can  operate  as  socialagents  capable  of  complex,  human-like  interactions  (Parket al. 2023). Their integration into online social platformsis  unfolding  rapidly  (Cao  et  al.  2025;  Yang  and  Menczer2024), presenting potentially severe risks (Floridi and Chiri-atti 2020; Ferrara 2024; Nogara et al. 2025)—especially dueto  their  biases  (Liu,  Bono,  and  Pierri  2025;  Saffari  et  al.2025)—as  well  as  intriguing  opportunities  (Dafoe  et  al.2020;  Breum  et  al.  2024),  for  example,  as  fact-checkingtools  (Fontana  et  al.  2025).  To  understand  and  anticipateCopyright © 2025, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.the  behavioral  dynamics  that  may  arise  from  the  interac-tion  between  artificial  agents  and  humans,  it  is  essentialto first study how these agents react to simple social stim-uli (Bail 2024). Behavioral economics experiments, partic-ularly  those  grounded  in  Game  Theory,  provide  an  idealground  for  testing  the  responses  of  AI  agents  to  archety-pal social situations (Horton 2023). These experiments typ-ically involve goal-oriented scenarios where multiple ‘play-ers’  engage  in  a  series  of  repeated  interactions  (Osborneand  Rubinstein  1994).  To  optimize  for  the  goal,  the  deci-sions taken at each round must strategically account for theanticipated actions of the other players. However, the deci-sions of human participants often deviate from the theoret-ically optimal strategies due to the influence of social andpsychological  factors  that  conflict  with  the  game’s  objec-tives (Camerer 1997). Similarly, given that LLMs are com-putational  models  built  upon  collective  human  knowledgeand culture (Schramowski et al. 2022), observing their be-havior in classic iterated games could shed light on the so-cial norms and values that these models reflect, as well astheir capability in reasoning, planning, and collaborating insocial settings.Early  interdisciplinary  research  has  explored  the  use  ofLLMs within the context of classical economic games (seeRelated Work). While highly valuable, all these studies ex-hibit  at  least  one  of  the  following  limitations.  First,  theygenerally lack prompt validation procedures, leading to animplicit assumption that LLMs can understand the complexrules of the game and the history of past actions describedin the prompt (Akata et al. 2023; Mao et al. 2025; Mei et al.2024). Second, the duration of simulated games is often lim-ited to a few rounds (Akata et al. 2023; Brookins and De-Backer 2024; Fan et al. 2024; Guo 2023; Xu et al. 2024),hampering the LLMs’ ability to discern the decision-makingpatterns of other participants—a phenomenon we show inour  own  experiments.  Third,  the  initialization  of  LLMswith predefined ‘personas’ tends to skew their responses to-wards pre-determined behaviors, such as altruism or selfish-ness (Brookins and DeBacker 2024; Fan et al. 2024; Guo2023;  Horton  2023;  Lor`e  and  Heydari  2024;  Phelps  andRussell 2023). This approach limits the exploration of theLLMs’ baseline behavior, which is crucial for understandingtheir inherent decision-making processes. Last, the evalua-tion of simulation outcomes has predominantly focused onProceedings of the Nineteenth International AAAI Conference on Web and Social Media (ICWSM2025)522

the quantitative analysis of decision types (e.g., frequencyof cooperation), overlooking the LLMs’ higher-level behav-ioral patterns that can be inferred from the temporal evolu-tion of these decisions (Xu et al. 2024; Mao et al. 2025). Thecombined effect of these limitations has led to findings thatare sometimes inconclusive (Brookins and DeBacker 2024;Mao et al. 2025) and contradictory (Akata et al. 2023; Fanet al. 2024), calling for more systematic evidence on the be-havior of LLMs in iterated games.In  this  work,  we  investigate  the  adaptability  of  LLMsin terms of their cooperative behavior when facing a spec-trum of hostility in an iterated game scenario. We evaluateLlama2 (Touvron et al. 2023), Llama3, and GPT3.5 (Brownet al. 2020) in the Iterated Prisoner’s Dilemma (Osborne andRubinstein 1994) against adversaries with different propen-sities for betrayal. Our contribution is threefold. First, we in-troduce a meta-prompting technique designed to evaluate anLLM’s comprehension of the game’s rules and its ability toparse historical gameplay logs for decision-making. Second,we conduct extensive simulations over 100 rounds and deter-mine the optimal memory span that enables the LLMs to ad-here to the strategic framework of the game. Third, we ana-lyze the behavioral patterns exhibited by the LLMs, aligningthem with the core dimensions and strategies delineated inRobert Axelrod’s influential research on the evolution of co-operation within strategic interactions (Axelrod and Hamil-ton 1981).We observe that, overall, the three models tend to be morecooperative than humans, but they display some variabilityin  their  strategies.  This  variability  persists  even  when  themodels are exposed to the same environment, game, and taskframing. Both Llama2 and GPT3.5 displayed a more markedpropensity towards cooperation than what existing literaturereports about human players, indicating a favorable align-ment with positive values. In contrast, Llama3 adopts a morestrategic  and  exploitative  approach  that  is  more  similar  tothat of humans. This approach may be advantageous in com-petitive scenarios where raw performance is critical, but itcan be a disadvantage when it comes to aligning with posi-tive values.Overall, our work contributes to defining a more princi-pled approach to using LLMs for iterated games. It makesa step towards a more systematic way to use simulations ofgame theoretical scenarios to probe the inherent social bi-ases of LLMs, which might prove useful for LLM auditingand alignment (Shen et al. 2023; M ̈okander et al. 2023).2    Background on Prisoner’s Dilemma2.1    Game SetupThe Prisoner’s Dilemma is a classic thought experiment inGame Theory. It serves as a paradigm for analyzing conflictand cooperation between two players (Tucker and Straffin Jr1983). In the game, the two players cannot communicate,and  must  independently  choose  between  two  actions:Co-operate, orDefect. Once both players have chosen their ac-tions, payoffs are distributed based on the resulting combi-nation of their choices. Mutual cooperation yields a rewardRfor  each  player.  If  one  defects  while  the  other  cooper-ates, the defector receives a higher ‘temptation’ payoffT,while the cooperating player incurs a lower ‘sucker’s’ pay-offS. If both parties choose to defect, they each receive apunishment  payoffPfor  failing  to  cooperate.  The  classi-cal structure of the game is defined by the payoff hierarchyT > R > P > S, which theoretically incentivizes rationalplayers to consistently choose defection as their dominantstrategy (Axelrod 1981). In the iterated version of the game,multiple rounds are played, and the payoffs are revealed tothe players at every round (Tucker and Luce 1959). The it-erative  nature  of  the  game  allows  the  players  to  considerpast outcomes to strategically inform future actions. Whenhumans play the game, psychological factors such as repu-tation and trust significantly influence the decision-makingprocess,  often  leading  to  higher  rates  of  cooperation  thanwould be expected from purely rational agents (Dal B ́o andFr ́echette 2011; Romero and Rosokha 2018).2.2    StrategiesIn the Iterated Prisoner’s Dilemma (IPD), astrategyrefersto an algorithm that a player uses to decide their next action,taking into account the historical context of the game (Kuhn2024).  No  single  strategy  universally  outperforms  all  oth-ers; however, some are more effective against a broader va-riety  of  opposing  strategies  (Dal  B ́o  and  Fr ́echette  2011).This  concept  was  demonstrated  in  1980  by  Robert  Axel-rod (Axelrod 1980), who ran an IPD tournament with mul-tiple competing strategies. Follow-up tournaments have fur-ther diversified the range of strategies (Stewart and Plotkin2012).  Considering  previous  literature  (Fudenberg,  Rand,and Dreber 2012; Dal B ́o and Fr ́echette 2011; Romero andRosokha 2018), we consider the strategies that better repre-sent the ones adopted by humans, covering more than75%of the experimental samples in those studies. Those strate-gies are the following:•Always Cooperate(AC).•Always Defect(AD).•Random(RND). Chooses Cooperate or Defect at randomwith equal probability at each round.•Unfair Random(URNDp).  Variation  of  Random  wherethe probability of choosing to Cooperate isp.•Tit For Tat(TFT).  Starts  with  Cooperation  in  the  firstround,   then   mimics   the   opponent’s   previous   actionthroughout the game.•Suspicious Tit For Tat(STFT). ATFTstrategy that be-gins with Defect in the first round.•Grim Trigger(GRIM). Chooses Cooperate until the oppo-nent defects, then chooses only Defect for the rest of thegame.•Win–Stay Lose–Shift(WSLS).  Repeats  the  previous  ac-tion if it resulted in the highest payoffs (RorT), otherwisechanges action.Tit For Tatemerged  as  the  winning  strategy  in  Axelrod’stournament.  It  is  commonly  observed  that  human  playerstend to favor straightforward strategies such asAD,TFT, orGRIM(Romero and Rosokha 2018). To describe the LLM523
