# T√≥m T·∫Øt Features M·ªõi - ƒê√£ Ho√†n Th√†nh ‚úÖ

## Ch√∫c M·ª´ng! üéâ

ƒê√£ th√™m th√†nh c√¥ng 2 features m·ªõi theo paper **"Nicer than Human: How Do Large Language Models Behave in the Prisoner's Dilemma?"**

---

## ‚úÖ Features ƒê√£ Implement

### 1. **Reasoning Extraction** üß†
L∆∞u l√Ω do suy nghƒ© c·ªßa agent v√†o JSON

**C√°ch ho·∫°t ƒë·ªông:**
- Agent ch·ªçn strategy (Cooperate/Defect)
- H·ªá th·ªëng h·ªèi th√™m: "T·∫°i sao b·∫°n ch·ªçn v·∫≠y?"
- Agent gi·∫£i th√≠ch reasoning
- L∆∞u v√†o JSON output

**K√≠ch ho·∫°t:**
```bash
python triad_experiment.py --game PD --models Qwen2.5-32B --rounds 5 --reasoning
```

---

### 2. **Meta-Prompting** üîç
Test xem agent c√≥ hi·ªÉu lu·∫≠t ch∆°i kh√¥ng (comprehension validation)

**3 lo·∫°i c√¢u h·ªèi validation:**
1. **Payoff Understanding** - Hi·ªÉu payoff structure kh√¥ng?
2. **History Recall** - Nh·ªõ ƒë∆∞·ª£c history kh√¥ng?
3. **Strategy Understanding** - Hi·ªÉu objective kh√¥ng?

**K√≠ch ho·∫°t:**
```bash
python triad_experiment.py --game PD --models Qwen2.5-32B --rounds 10 --meta-prompt
```

---

## üìä Output JSON M·ªõi

### V·ªõi Reasoning (--reasoning)
```json
{
  "round_3": [
    {
      "agent": "Alice",
      "strategy": "Cooperate",
      "score": 7,
      "reasoning": "I cooperated because Bob cooperated in the last 2 rounds."
    }
  ]
}
```

### V·ªõi Meta-Prompting (--meta-prompt)
```json
{
  "round_1": [
    {
      "agent": "Alice",
      "strategy": "Cooperate",
      "score": 7,
      "reasoning": "Starting with cooperation to build trust.",
      "meta_prompt_validation": {
        "payoff_understanding": "If I cooperate and opponent defects, I get sucker's payoff.",
        "history_recall": "N/A (Round 1)",
        "strategy_understanding": "Maximize my total score across all rounds."
      }
    }
  ]
}
```

---

## ‚úÖ Test Results

**T·∫•t c·∫£ 4 tests ƒë·ªÅu PASS:**

```
[PASS]: Reasoning Extraction
[PASS]: Meta-Prompting
[PASS]: Combined Features
[PASS]: Output Format
```

**Ch·∫°y test:**
```bash
cd Project_Triad
python test_new_features.py
```

---

## üöÄ C√°ch S·ª≠ D·ª•ng

### Example 1: Reasoning Only
```bash
python triad_experiment.py \
  --game PD \
  --models Qwen2.5-32B \
  --rounds 10 \
  --reasoning
```

### Example 2: Meta-Prompting Only
```bash
python triad_experiment.py \
  --game PD \
  --models Qwen2.5-32B \
  --rounds 10 \
  --meta-prompt \
  --meta-rounds "1,5,10"
```

### Example 3: Both Features (Full Package)
```bash
python triad_experiment.py \
  --game PD \
  --models Qwen2.5-32B \
  --rounds 20 \
  --reasoning \
  --meta-prompt
```

### Example 4: Replicate "Nicer than Human" Paper
```bash
python triad_experiment.py \
  --game PD \
  --models "Llama3-70B,Qwen2.5-72B" \
  --rounds 100 \
  --languages en \
  --reasoning \
  --meta-prompt \
  --meta-rounds "1,10,25,50,75,100"
```

---

## üìÅ Files T·∫°o/S·ª≠a

### ƒê√£ S·ª≠a
- `triad_experiment.py` - Th√™m reasoning + meta-prompting logic

### ƒê√£ T·∫°o
- `NEW_FEATURES.md` - Chi ti·∫øt v·ªÅ features m·ªõi
- `test_new_features.py` - Test script
- `SUMMARY_VI.md` - T√†i li·ªáu n√†y

---

## ‚ö° Performance Impact

| Feature | Extra Time per Round | Extra LLM Calls |
|---------|---------------------|-----------------|
| Reasoning | ~3-5s | +3 (1 per agent) |
| Meta-Prompting | ~10-15s | +9 (3 questions √ó 3 agents) |
| Both | ~15-20s | +12 total |

**L∆∞u √Ω:** Meta-prompting ch·ªâ ch·∫°y ·ªü rounds ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh

---

## üìñ T√†i Li·ªáu

### ƒê·ªçc Chi Ti·∫øt
- `NEW_FEATURES.md` - Chi ti·∫øt technical implementation
- `QUICK_START.md` - Quick reference commands
- `FIXES.md` - Bug fixes documentation

### Paper Reference
- **Title**: "Nicer than Human: How Do Large Language Models Behave in the Prisoner's Dilemma?"
- **Conference**: ICWSM 2025
- **Authors**: Nicol√≤ Fontana, Francesco Pierri, Luca Maria Aiello

---

## üéØ Next Steps

### Immediate Usage
```bash
# Test ngay v·ªõi MockModel (kh√¥ng c·∫ßn GPU)
python triad_experiment.py --game PD --models MockModel --rounds 5 --reasoning --meta-prompt

# Ch·∫°y v·ªõi model th·∫≠t
python triad_experiment.py --game PD --models Qwen2.5-32B --rounds 20 --reasoning --meta-prompt
```

### Future Enhancements
1. **Behavioral Analysis** - T·ª± ƒë·ªông ph√¢n t√≠ch reasoning patterns
2. **Strategy Classification** - Detect TFT, GRIM, etc. t·ª´ reasoning
3. **Visualization** - Plot reasoning evolution over time
4. **Multi-language** - Support Vietnamese meta-prompts

---

## üí° Tips & Tricks

### Tip 1: Ki·ªÉm Tra Comprehension
```bash
# Ch·ªâ ch·∫°y meta-prompting ·ªü round ƒë·∫ßu
python triad_experiment.py --game PD --models Qwen2.5-32B --rounds 5 --meta-prompt --meta-rounds "1"
```
Xem output ƒë·ªÉ verify agent c√≥ hi·ªÉu game kh√¥ng tr∆∞·ªõc khi ch·∫°y 100 rounds.

### Tip 2: L·ªçc Reasoning
```python
import json

with open('results.json', 'r') as f:
    data = json.load(f)

# Extract all reasoning
for exp_name, exp_data in data.items():
    for round_key, round_data in exp_data['history'].items():
        for agent in round_data:
            if 'cooperat' in agent['reasoning'].lower():
                print(f"{agent['agent']}: {agent['reasoning']}")
```

### Tip 3: Compare Models
```bash
# Compare 3 models
python triad_experiment.py \
  --game PD \
  --models "Qwen2.5-32B,Llama3-70B,GPT3.5" \
  --rounds 20 \
  --reasoning \
  --meta-prompt
```

Sau ƒë√≥ analyze xem model n√†o reasoning t·ªët h∆°n.

---

## ‚ùì FAQ

**Q: C√≥ b·∫Øt bu·ªôc ph·∫£i d√πng reasoning kh√¥ng?**
A: Kh√¥ng. M·∫∑c ƒë·ªãnh t·∫Øt. Ch·ªâ b·∫≠t khi c·∫ßn analyze decision-making.

**Q: Meta-prompting l√†m ch·∫≠m nhi·ªÅu kh√¥ng?**
A: Ch·ªâ ch·∫°m ·ªü rounds ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh. N·∫øu set `--meta-rounds "1,10"` th√¨ ch·ªâ 2 rounds b·ªã ·∫£nh h∆∞·ªüng.

**Q: C√≥ th·ªÉ d√πng v·ªõi Volunteer Dilemma kh√¥ng?**
A: C√≥! M·ªçi game ƒë·ªÅu support:
```bash
python triad_experiment.py --game VD --models Qwen2.5-32B --reasoning --meta-prompt
```

**Q: MockModel c√≥ reasoning kh√¥ng?**
A: C√≥, nh∆∞ng random. Ch·ªâ d√πng ƒë·ªÉ test structure, kh√¥ng ph√¢n t√≠ch content.

**Q: L√†m sao analyze reasoning quality?**
A: Xem `NEW_FEATURES.md` section "Ph√¢n T√≠ch K·∫øt Qu·∫£" c√≥ Python script m·∫´u.

---

## üéä K·∫øt Lu·∫≠n

**B·∫°n ƒë√£ c√≥:**
‚úÖ Reasoning extraction trong JSON  
‚úÖ Meta-prompting validation  
‚úÖ Full test suite (100% pass)  
‚úÖ Documentation ƒë·∫ßy ƒë·ªß  
‚úÖ Example commands s·∫µn s√†ng d√πng  

**S·∫µn s√†ng ch·∫°y experiment!** üöÄ

```bash
# Quick test
python triad_experiment.py --game PD --models MockModel --rounds 3 --reasoning --meta-prompt

# Real experiment
python triad_experiment.py --game PD --models Qwen2.5-32B --rounds 100 --reasoning --meta-prompt
```

---

**Ch√∫c th√≠ nghi·ªám th√†nh c√¥ng! üéâ**

N·∫øu c·∫ßn h·ªó tr·ª£, xem:
- `NEW_FEATURES.md` - Technical details
- `QUICK_START.md` - Command reference
- `test_new_features.py` - Code examples

